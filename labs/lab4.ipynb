{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGzC3uqmuKZB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 4: Q-table based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYeKUsX8uXSF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "Solve [`FrozenLake8x8-v1`](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) using a Q-table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGAOGNSWyncb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Import Necessary Packages (e.g. `gym`, `numpy`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7KHXZDxys6J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake8x8-v1\")#render_mode='human'\n",
    "environment.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMs2BVFZywAJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Set up the QTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "actions = environment.action_space.n\n",
    "states = environment.observation_space.n\n",
    "q_table = np.zeros((states, actions))\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHuDteJVy2_C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. The Q-Learning algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.82035770e-04 9.06889640e-04 2.34150506e-03 1.02062296e-03]\n",
      " [1.18197249e-03 1.17353393e-03 2.54318888e-03 1.40086166e-03]\n",
      " [1.69532783e-03 1.70096880e-03 3.55864767e-03 1.82774410e-03]\n",
      " [2.60115528e-03 3.25747410e-03 5.42714017e-03 2.61491865e-03]\n",
      " [4.15871334e-03 4.91554730e-03 7.53403397e-03 4.13146553e-03]\n",
      " [6.22801129e-03 6.90186016e-03 1.00259097e-02 6.22512885e-03]\n",
      " [8.17537171e-03 6.55918899e-03 1.23445419e-02 5.69833791e-03]\n",
      " [8.51691402e-03 1.34909315e-02 8.54695543e-03 8.50685921e-03]\n",
      " [8.72562443e-04 8.69668860e-04 8.73947608e-04 1.95169820e-03]\n",
      " [1.13229455e-03 1.13939312e-03 1.09463644e-03 2.38123919e-03]\n",
      " [1.42275047e-03 1.37608273e-03 1.46968589e-03 3.37395328e-03]\n",
      " [1.28828421e-03 2.00027605e-03 1.93695841e-03 4.71261252e-03]\n",
      " [2.62644646e-03 2.79639438e-03 3.49213629e-03 7.13024845e-03]\n",
      " [2.33382835e-03 3.74314675e-03 5.48977725e-03 1.04364265e-02]\n",
      " [8.35205126e-03 7.83914022e-03 1.73079395e-02 5.91822571e-03]\n",
      " [1.03570732e-02 8.78482798e-03 1.93941291e-02 8.79524486e-03]\n",
      " [7.76015188e-04 7.74652654e-04 7.52949346e-04 8.22138837e-04]\n",
      " [8.58309291e-04 7.49604699e-04 8.67700634e-04 9.40454764e-04]\n",
      " [1.03276754e-03 2.92946267e-04 7.48104596e-04 8.92067205e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.20363750e-03 9.04968218e-04 7.84630989e-04 0.00000000e+00]\n",
      " [8.04827687e-04 7.73644683e-03 1.77099426e-03 1.30149291e-03]\n",
      " [3.53069897e-03 1.30192810e-02 2.42817381e-02 2.85871783e-03]\n",
      " [9.27728357e-03 2.91588613e-02 1.35309825e-02 7.81232996e-03]\n",
      " [5.11437050e-04 3.91446833e-04 4.72470328e-04 6.42500739e-04]\n",
      " [4.90703376e-04 3.49906267e-04 4.82332637e-04 6.15434389e-04]\n",
      " [5.88672380e-04 9.33651845e-05 8.72443626e-05 2.04387049e-04]\n",
      " [0.00000000e+00 4.03856669e-04 0.00000000e+00 2.12758351e-05]\n",
      " [3.10280338e-06 0.00000000e+00 1.15661382e-04 4.81250408e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.78794634e-02 1.03791433e-02]\n",
      " [3.78580746e-03 4.73926365e-02 1.50382508e-02 8.75945354e-03]\n",
      " [3.52134603e-04 1.24552277e-04 2.11316916e-04 1.69275973e-04]\n",
      " [2.65931644e-04 7.79559788e-05 9.31450127e-05 4.08571836e-04]\n",
      " [3.52554558e-04 4.18144947e-13 2.32349397e-05 6.35520646e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.18736878e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.39909924e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.35645928e-03 0.00000000e+00 8.77130719e-03 3.72225954e-02]\n",
      " [7.27284662e-02 1.26363455e-02 2.13267967e-02 3.15620077e-03]\n",
      " [2.17600506e-04 5.46447074e-06 3.77161717e-05 2.26191664e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.23034087e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.04062091e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.86276862e-01 0.00000000e+00 5.00716889e-02 1.45163533e-02]\n",
      " [1.56225993e-04 0.00000000e+00 0.00000000e+00 1.34889569e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.90156949e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.10000000e-04 0.00000000e+00 6.39761880e-01 0.00000000e+00]\n",
      " [1.17746076e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.85824634e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000 #Total amount of episodes\n",
    "max_steps = 100 #Max steps per episode\n",
    "learning_rate = 0.1  #Learning rate\n",
    "randomness = 1.0 #Exploratoion rate\n",
    "max_randomness = 1.0 #Exploration propability at start\n",
    "min_randomness = 0.01 #Min exploration\n",
    "zombie_rate = 0.001 #Decay rate\n",
    "gamma = 0.9 #Discounting rate \n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = environment.reset()[0] #Resets and collects the first state in the env\n",
    "    finished = False\n",
    "    all_rewards = 0\n",
    "    moves = 0\n",
    "    if episode % 200 == 0:#Saves every 200th  \n",
    "        np.savetxt(\"Q_Table\", q_table)\n",
    "    random_value = random.random()\n",
    "    for moves in range(max_steps):\n",
    "        #random_value = random.random()\n",
    "        #If its greather than randomness, then we're taking the biggest Q value\n",
    "        if random_value > randomness:\n",
    "            action = np.argmax(q_table[state, :]) #Finds the highest Qvalue which matches the state we have. \n",
    "        #If its not, then we're doing a random choice\n",
    "        else: \n",
    "            action = environment.action_space.sample()\n",
    "        #Does the action    \n",
    "        new_state, reward, finished, _ , _ = environment.step(action)\n",
    "        #Updating our Qtable \n",
    "        q_table[state,action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        all_rewards += reward\n",
    "        state = new_state #Update our current state\n",
    "        if finished:\n",
    "            break \n",
    "    #We're reducing randomness, as we need less exploration over time   \n",
    "    randomness = min_randomness + (max_randomness - min_randomness) * np.exp(- zombie_rate * episode)\n",
    "    rewards.append(all_rewards)\n",
    "print(q_table)\n",
    "print(np.average(new_state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm8oigYjzFTd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. Evaluate how well your agent performs\n",
    "* Render output of one episode\n",
    "* Give an average episode return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake8x8-v1\", render_mode='human')\n",
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps 25\n",
      "35.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1):\n",
    "    state = environment.reset()[0]\n",
    "    finished = False\n",
    "    moves = 0\n",
    "    for moves in range(max_steps):\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, finished, _ , _ = environment.step(action)\n",
    "        if finished:\n",
    "            #If its done with an episode, it will render the output\n",
    "            environment.render()\n",
    "            print(\"Number of steps\", moves)\n",
    "            break\n",
    "        state = new_state\n",
    "environment.close()\n",
    "#Average episode return\n",
    "print(np.average(new_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6. (<i>Optional</i>) Adapt code for one of the continuous [Classical Control](https://www.gymlibrary.dev/environments/classic_control/) problems. Think/talk about how you could use our  `Model` class from last Thursday to decide actions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtNkS92UHFInFg+R4UDAlq",
   "name": "Reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1100cb1e8002ed1bdf0f722445998702f8f2abac26682cdcd9eca184988adcbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
